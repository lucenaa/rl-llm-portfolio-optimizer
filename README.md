# Agente H√≠brido (RL+LLM) para Otimiza√ß√£o de Portf√≥lio

Este reposit√≥rio cont√©m a implementa√ß√£o de um agente aut√¥nomo para otimiza√ß√£o din√¢mica de portf√≥lios financeiros. O projeto utiliza uma abordagem h√≠brida, combinando Aprendizado por Refor√ßo (RL) para a tomada de decis√µes estrat√©gicas e um Modelo de Linguagem Grande (LLM) para extrair insights de not√≠cias financeiras.

## üìú Vis√£o Geral

O objetivo deste projeto √© desenvolver e avaliar um agente que aprende a gerenciar um portf√≥lio de ativos (SPY, QQQ, GLD) de forma aut√¥noma. Diferentemente de abordagens tradicionais, este agente enriquece sua percep√ß√£o do mercado com uma an√°lise de sentimento de not√≠cias, realizada pelo Google Gemini, para tomar decis√µes mais informadas.

A estrat√©gia aprendida pelo agente n√£o foca em maximizar o lucro a qualquer custo, mas em otimizar o retorno ajustado pelo risco, buscando um crescimento de capital mais est√°vel e com prote√ß√£o contra grandes perdas.

## ‚ú® Principais Features

- **Agente H√≠brido**: Sinergia entre o algoritmo PPO (Proximal Policy Optimization) do Aprendizado por Refor√ßo e a capacidade de interpreta√ß√£o do Google Gemini.
- **An√°lise de Sentimento**: Extra√ß√£o de um score de sentimento de not√≠cias financeiras para alimentar o estado do agente.
- **Ambiente de Backtesting Realista**: Simula√ß√£o de mercado com custos de transa√ß√£o para uma avalia√ß√£o mais precisa.
- **An√°lise de Performance**: Compara√ß√£o de m√©tricas financeiras chave (Sharpe Ratio, Max Drawdown, Retorno Total) contra um benchmark passivo (Buy & Hold).

## ‚öôÔ∏è Arquitetura do Projeto

O fluxo de trabalho do projeto √© dividido em cinco scripts sequenciais:

1. **`1_data_collection.py`**: Coleta de dados hist√≥ricos de pre√ßos via yfinance e cria√ß√£o de um dataset simulado de not√≠cias.
2. **`2_feature_extraction.py`**: Utiliza√ß√£o da API do Google Gemini para processar as not√≠cias e gerar um score de sentimento, salvando o resultado.
3. **`3_rl_environment.py`**: Defini√ß√£o do ambiente de simula√ß√£o customizado (Gymnasium), onde o agente ir√° treinar e ser avaliado.
4. **`4_train_agent.py`**: Treinamento do agente PPO utilizando 80% dos dados hist√≥ricos. O modelo treinado (o "c√©rebro" do agente) √© salvo em um arquivo .zip.
5. **`5_evaluate_agent.py`**: Avalia√ß√£o do agente treinado nos 20% de dados restantes (nunca vistos antes), gerando as m√©tricas de desempenho e o gr√°fico final.

## üõ†Ô∏è Tecnologias Utilizadas

- **Linguagem**: Python 3.10+
- **Aprendizado por Refor√ßo**: Stable Baselines3 (com PyTorch) & Gymnasium
- **An√°lise de Dados**: Pandas & NumPy
- **Coleta de Dados**: YFinance
- **Modelo de Linguagem**: Google Gemini API
- **Visualiza√ß√£o**: Matplotlib

## üöÄ Instala√ß√£o e Configura√ß√£o

Siga os passos abaixo para configurar o ambiente e rodar o projeto localmente.

### 1. Clone o reposit√≥rio:

```bash
git clone https://github.com/lucenaa/rl-llm-portfolio-optimizer
cd rl-llm-portfolio-optimizer
```

### 2. Crie e ative um ambiente virtual:

```bash
python3 -m venv venv
source venv/bin/activate
# No Windows, use: venv\Scripts\activate
```

### 3. Instale as depend√™ncias:
(Certifique-se de que voc√™ criou o arquivo `requirements.txt`).

```bash
pip install -r requirements.txt
```

### 4. Configure sua chave de API:

- Crie um arquivo chamado `.env` na raiz do projeto.
- Adicione sua chave da API do Google Gemini a este arquivo, da seguinte forma:

```
GEMINI_API_KEY=SUA_CHAVE_API_AQUI
```

## ‚ñ∂Ô∏è Como Executar

Execute os scripts na ordem correta. Cada script realiza uma etapa do processo.

### 1. Coletar dados de pre√ßos e not√≠cias:

```bash
python3 1_data_collection.py
```

### 2. Extrair sentimento com o LLM (requer a chave de API):

```bash
python3 2_feature_extraction.py
```

### 3. Treinar o agente de RL (pode levar alguns minutos):

```bash
python3 4_train_agent.py
```

### 4. Avaliar o agente e gerar os resultados:

```bash
python3 5_evaluate_agent.py
```

## üìä Resultados

O agente foi avaliado em um per√≠odo de teste de ~8 meses (Mai/2023 a Jan/2024). Os resultados demonstram que, embora a estrat√©gia passiva de Buy & Hold tenha gerado um retorno bruto maior durante este forte mercado de alta, o agente demonstrou uma efici√™ncia de risco-retorno significativamente superior.

| M√©trica | Agente RL+LLM | Benchmark (Buy & Hold SPY) |
|---------|---------------|---------------------------|
| **Retorno Total** | 17.86% | ~ 26% |
| **Maximum Drawdown** | -7.69% | ~ -8% |
| **Sharpe Ratio (Anual.)** | 2.33 | ~ 1.8 |

A principal conclus√£o √© que o agente aprendeu com sucesso a priorizar a consist√™ncia e a gest√£o de risco, validando a abordagem h√≠brida proposta.

## üîÆ Trabalhos Futuros

- [ ] **Otimiza√ß√£o de Hiperpar√¢metros**: Utilizar ferramentas como Optuna para encontrar a melhor combina√ß√£o de par√¢metros para o algoritmo PPO.
- [ ] **Features Avan√ßadas de LLM**: Extrair informa√ß√µes mais complexas que apenas sentimento, como identifica√ß√£o de eventos ou t√≥picos macroecon√¥micos.
- [ ] **Avalia√ß√£o em Diferentes Regimes**: Testar o desempenho do agente em dados de mercados de baixa (bear market) ou laterais.
- [ ] **Modelagem de Risco Alternativa**: Explorar fun√ß√µes de recompensa baseadas em outras m√©tricas, como o Sortino Ratio.

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT. Veja o arquivo LICENSE para mais detalhes.
